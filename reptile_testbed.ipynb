{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        # have to set layer weights to be doubles as usually initialised as floats\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 40).double(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40).double(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 2).double()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model, task, lr, iterations, criterion):\n",
    "    # load weights\n",
    "    # model.load_state_dict(state_dict)\n",
    "    \n",
    "    # set to training mode\n",
    "    model.train()\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr)\n",
    "    for i in range(iterations):\n",
    "        # get training points\n",
    "        for data in task:\n",
    "            y, x = data[:, 0], data[:, 1:]\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "    return model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_banana(file, num_splits, column='x1'):\n",
    "    data_ = pd.read_csv(file)\n",
    "    # replaces -1s with 0s\n",
    "    data_.y.replace(-1.0, 0, inplace=True)\n",
    "    # converts labels to int64 which is required by cross entropy loss\n",
    "    data_['y'] = data_['y'].astype('int64')\n",
    "    data_size = len(data_)\n",
    "\n",
    "    # sort by x1\n",
    "    data_ = data_.sort_values(column)\n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(32)\n",
    "\n",
    "    # split into train, test splits\n",
    "    split_size = np.floor(len(data_) / num_splits).astype(int)\n",
    "    ratio = 0.75\n",
    "    idx = 0\n",
    "    split_len = []\n",
    "    \n",
    "    data = {}\n",
    "    for i in range(num_splits):\n",
    "        if i < 2:\n",
    "            # draw indices\n",
    "            test_idx = np.random.choice(split_size, np.floor(ratio * split_size).astype(int), replace=False)\n",
    "            # create boolean mask\n",
    "            mask = np.array([True if j in test_idx else False for j in np.arange(split_size)])\n",
    "            rev_mask = np.invert(mask)\n",
    "            split_len.append(sum(rev_mask))\n",
    "            data[str(i) + '_train'] = torch.from_numpy(data_[idx:split_size+idx].values[rev_mask, :])\n",
    "            data[str(i) + '_test'] = torch.from_numpy(data_[idx:split_size+idx].values[mask, :])\n",
    "            idx += split_size\n",
    "        else:\n",
    "             # draw indices\n",
    "            test_idx = np.random.choice(len(data_[idx:]), np.floor(ratio * len(data_[idx:])).astype(int), replace=False)\n",
    "            # create boolean mask\n",
    "            mask = np.array([True if j in test_idx else False for j in np.arange(len(data_[idx:]))])\n",
    "            rev_mask = np.invert(mask)\n",
    "            split_len.append(sum(rev_mask))\n",
    "            data[str(i) + '_train'] = torch.from_numpy(data_[idx:].values[rev_mask, :])\n",
    "            data[str(i) + '_test'] = torch.from_numpy(data_[idx:].values[mask, :])\n",
    "            \n",
    "    return data, split_len\n",
    "\n",
    "def gen_task_data(data, split_len, num_splits, batch_size):\n",
    "    # select task\n",
    "    task_num = int(torch.randint(num_splits, (1, )).item())\n",
    "    \n",
    "    # create dataloader to sample from task\n",
    "    loader = DataLoader(\n",
    "        data[str(task_num) + '_train'],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=False\n",
    "        )\n",
    "    \n",
    "    return loader, split_len[task_num] / batch_size\n",
    "\n",
    "def gen_eval_data(data, num_splits):\n",
    "    for split in np.arange(num_splits):\n",
    "        if split == 0:\n",
    "            test_data = data[str(split) + '_test']\n",
    "        else:\n",
    "            test_data = torch.cat([test_data, data[str(split) + '_test']])\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reptile_1_step(model, task, o_lr, i_lr, i_iters, criterion):\n",
    "    # create a copy of weights to be used in outer loop\n",
    "    weights_main = deepcopy(model.state_dict())\n",
    "   \n",
    "    # run inner loop\n",
    "    for _ in torch.arange(i_iters):\n",
    "        weights = sgd(model, task, i_lr, i_iters, criterion)\n",
    "    \n",
    "    # update model's state_dict()\n",
    "    for key in weights_main.keys():\n",
    "        weights_main[key] += o_lr * (weights[key] - weights_main[key])\n",
    "        \n",
    "    return weights_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration tensor(0.):  tensor(0.6966, dtype=torch.float64)\n",
      "Iteration tensor(20.):  tensor(0.6938, dtype=torch.float64)\n",
      "Iteration tensor(40.):  tensor(0.6910, dtype=torch.float64)\n",
      "Iteration tensor(60.):  tensor(0.6878, dtype=torch.float64)\n",
      "Iteration tensor(80.):  tensor(0.6844, dtype=torch.float64)\n",
      "Iteration tensor(100.):  tensor(0.6806, dtype=torch.float64)\n",
      "Iteration tensor(120.):  tensor(0.6766, dtype=torch.float64)\n",
      "Iteration tensor(140.):  tensor(0.6724, dtype=torch.float64)\n",
      "Iteration tensor(160.):  tensor(0.6681, dtype=torch.float64)\n",
      "Iteration tensor(180.):  tensor(0.6635, dtype=torch.float64)\n",
      "Iteration tensor(200.):  tensor(0.6588, dtype=torch.float64)\n",
      "Iteration tensor(220.):  tensor(0.6544, dtype=torch.float64)\n",
      "Iteration tensor(240.):  tensor(0.6498, dtype=torch.float64)\n",
      "Iteration tensor(260.):  tensor(0.6450, dtype=torch.float64)\n",
      "Iteration tensor(280.):  tensor(0.6414, dtype=torch.float64)\n",
      "Iteration tensor(300.):  tensor(0.6377, dtype=torch.float64)\n",
      "Iteration tensor(320.):  tensor(0.6330, dtype=torch.float64)\n",
      "Iteration tensor(340.):  tensor(0.6291, dtype=torch.float64)\n",
      "Iteration tensor(360.):  tensor(0.6242, dtype=torch.float64)\n",
      "Iteration tensor(380.):  tensor(0.6207, dtype=torch.float64)\n",
      "Iteration tensor(400.):  tensor(0.6171, dtype=torch.float64)\n",
      "Iteration tensor(420.):  tensor(0.6143, dtype=torch.float64)\n",
      "Iteration tensor(440.):  tensor(0.6105, dtype=torch.float64)\n",
      "Iteration tensor(460.):  tensor(0.6078, dtype=torch.float64)\n",
      "Iteration tensor(480.):  tensor(0.6057, dtype=torch.float64)\n",
      "Iteration tensor(500.):  tensor(0.6031, dtype=torch.float64)\n",
      "Iteration tensor(520.):  tensor(0.5992, dtype=torch.float64)\n",
      "Iteration tensor(540.):  tensor(0.5981, dtype=torch.float64)\n",
      "Iteration tensor(560.):  tensor(0.5965, dtype=torch.float64)\n",
      "Iteration tensor(580.):  tensor(0.5936, dtype=torch.float64)\n",
      "Iteration tensor(600.):  tensor(0.5924, dtype=torch.float64)\n",
      "Iteration tensor(620.):  tensor(0.5903, dtype=torch.float64)\n",
      "Iteration tensor(640.):  tensor(0.5890, dtype=torch.float64)\n",
      "Iteration tensor(660.):  tensor(0.5863, dtype=torch.float64)\n",
      "Iteration tensor(680.):  tensor(0.5868, dtype=torch.float64)\n",
      "Iteration tensor(700.):  tensor(0.5871, dtype=torch.float64)\n",
      "Iteration tensor(720.):  tensor(0.5872, dtype=torch.float64)\n",
      "Iteration tensor(740.):  tensor(0.5877, dtype=torch.float64)\n",
      "Iteration tensor(760.):  tensor(0.5859, dtype=torch.float64)\n",
      "Iteration tensor(780.):  tensor(0.5913, dtype=torch.float64)\n",
      "Iteration tensor(800.):  tensor(0.5910, dtype=torch.float64)\n",
      "Iteration tensor(820.):  tensor(0.5912, dtype=torch.float64)\n",
      "Iteration tensor(840.):  tensor(0.5946, dtype=torch.float64)\n",
      "Iteration tensor(860.):  tensor(0.5973, dtype=torch.float64)\n",
      "Iteration tensor(880.):  tensor(0.5992, dtype=torch.float64)\n",
      "Iteration tensor(900.):  tensor(0.5997, dtype=torch.float64)\n",
      "Iteration tensor(920.):  tensor(0.6006, dtype=torch.float64)\n",
      "Iteration tensor(940.):  tensor(0.6014, dtype=torch.float64)\n",
      "Iteration tensor(960.):  tensor(0.6080, dtype=torch.float64)\n",
      "Iteration tensor(980.):  tensor(0.6158, dtype=torch.float64)\n",
      "Final Test Loss tensor(999.):  tensor(0.6163, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Set num_splits\n",
    "num_splits = 3\n",
    "banana, split_len = load_banana('banana_data.csv', num_splits)\n",
    "test_data = gen_eval_data(banana, num_splits)\n",
    "y_test, x_test = test_data[:, 0], test_data[:, 1:]\n",
    "\n",
    "# Initialise model\n",
    "model = NN_Classifier()\n",
    "weights = model.state_dict()\n",
    "\n",
    "# Set hyperparameters\n",
    "o_iters = 1000\n",
    "o_lr = 0.001\n",
    "i_iters = 20\n",
    "i_lr = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for _ in torch.arange(o_iters):\n",
    "    # sample task\n",
    "    task, scale = gen_task_data(banana, split_len, num_splits, batch_size)\n",
    "    \n",
    "    # run reptile_1_step\n",
    "    weights = reptile_1_step(model, task, o_lr, scale * i_lr, i_iters, criterion)\n",
    "    \n",
    "    # re-assign weights to model\n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    if _ % 20 == 0:\n",
    "        y_hat = model(x_test)\n",
    "        test_loss = criterion(y_hat, y_test)\n",
    "        print('Iteration ' + str(_.item()) + ': ', test_loss.item())\n",
    "        \n",
    "y_hat = model(x_test)\n",
    "test_loss = criterion(y_hat, y_test)\n",
    "print('Final Test Loss ' + str(_.item()) + ': ', test_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
